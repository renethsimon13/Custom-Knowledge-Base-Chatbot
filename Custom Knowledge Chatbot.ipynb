{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required modules, functions and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r C:\\Programming\\Gadgeon\\requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install numpy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import util\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the embedding model and the locally downloaded LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self, document_path, model_name='msmarco-distilbert-base-tas-b', max_tokens=100, top_k=3):\n",
    "        self.document_path = document_path\n",
    "        self.model_name = model_name\n",
    "        self.max_tokens = max_tokens\n",
    "        self.top_k = top_k\n",
    "        self.tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.texts = []\n",
    "        self.sentences = []\n",
    "        self.embeddings = None\n",
    "        self.context = \"\"\n",
    "\n",
    "    def read_document(self):\n",
    "        with open(self.document_path, 'r') as f:\n",
    "            self.texts = f.read()\n",
    "\n",
    "    def tokenize_sentences(self):\n",
    "        self.sentences = self.texts.split('.')\n",
    "\n",
    "    def calculate_tokens(self):\n",
    "        n_tokens = [len(self.tokenizer.encode(\" \" + sentence)) for sentence in self.sentences]\n",
    "        return n_tokens\n",
    "\n",
    "    def split_sentences(self):\n",
    "        chunks = []\n",
    "        tokens_so_far = 0\n",
    "        chunk = []\n",
    "\n",
    "        n_tokens = self.calculate_tokens()\n",
    "\n",
    "        for sentence, token in zip(self.sentences, n_tokens):\n",
    "            if token + tokens_so_far > self.max_tokens:\n",
    "                chunks.append(\". \".join(chunk) + \".\")\n",
    "                chunk = []\n",
    "                tokens_so_far = 0\n",
    "\n",
    "            if token > self.max_tokens:\n",
    "                continue\n",
    "\n",
    "            chunk.append(sentence)\n",
    "            tokens_so_far += token + 1\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def create_dataframe(self, chunks):\n",
    "        df = pd.DataFrame(chunks, columns=['text'])\n",
    "        df['n_tokens'] = df.text.apply(lambda x: len(self.tokenizer.encode(x)))\n",
    "        return df\n",
    "\n",
    "    def encode_sentences(self):\n",
    "        sentences = self.df['text'].tolist()\n",
    "        self.embeddings = self.model.encode(sentences)\n",
    "\n",
    "    def encode_question(self, question):\n",
    "        embed_q = self.model.encode(question)\n",
    "        return embed_q\n",
    "\n",
    "    def find_top_results(self, embed_q):\n",
    "        cosine_scores = util.cos_sim(embed_q, self.embeddings)[0]\n",
    "        top_results = torch.topk(cosine_scores, k=self.top_k)\n",
    "        return top_results\n",
    "\n",
    "    def generate_context(self, top_results):\n",
    "        self.context = \"\"\n",
    "        for i in top_results.indices:\n",
    "            self.context += self.df['text'][i.item()] + \" \"\n",
    "\n",
    "    def process(self, question):\n",
    "        self.read_document()\n",
    "        self.tokenize_sentences()\n",
    "        chunks = self.split_sentences()\n",
    "        self.df = self.create_dataframe(chunks)\n",
    "        self.encode_sentences()\n",
    "        embed_q = self.encode_question(question)\n",
    "        top_results = self.find_top_results(embed_q)\n",
    "        self.generate_context(top_results)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processor = TextProcessor(\"C:\\Programming\\documents.txt\")\n",
    "    question = \"what is NLP?\"\n",
    "    processor.process(question)\n",
    "    print(\"Context: \\n{}\".format(processor.context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LanguageModelProcessor:\n",
    "    def __init__(self, model_path, n_gpu_layers=10, n_batch=512):\n",
    "        self.model_path = model_path\n",
    "        self.n_gpu_layers = n_gpu_layers\n",
    "        self.n_batch = n_batch\n",
    "        self.callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        self.llm = None\n",
    "        self.llm_chain = None\n",
    "        self.template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "        Answer:\"\"\"\n",
    "        self.prompt = PromptTemplate(template=self.template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "    def initialize_llm(self):\n",
    "        self.llm = LlamaCpp(\n",
    "            model_path=self.model_path,\n",
    "            n_gpu_layers=self.n_gpu_layers,\n",
    "            n_batch=self.n_batch,\n",
    "            callback_manager=self.callback_manager,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    def initialize_llm_chain(self):\n",
    "        self.llm_chain = LLMChain(prompt=self.prompt, llm=self.llm)\n",
    "\n",
    "    def get_llm_response(self, context, question):\n",
    "        response = self.llm_chain.run({\"context\": context, \"question\": question})\n",
    "        return response\n",
    "\n",
    "    def process(self, context, question):\n",
    "        self.initialize_llm()\n",
    "        self.initialize_llm_chain()\n",
    "        response = self.get_llm_response(context, question)\n",
    "\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation and Indexing of Embedding Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text_processor = TextProcessor(\"C:\\Programming\\documents.txt\")\n",
    "    question = \"what's NLP\"\n",
    "    text_processor.process(question)\n",
    "    context = text_processor.context\n",
    "    print(\"Context: \\n{}\".format(context))\n",
    "\n",
    "    processor = LanguageModelProcessor(model_path=\"C:\\Programming\\Gadgeon\\GPT4All-13B-snoozy.ggmlv3.q4_0.bin\")\n",
    "    response = processor.process(context, question)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running of query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = input(\"enter query\")\n",
    "   \n",
    "docsearch = Chroma.from_documents(splitting.texts, embeddings)\n",
    "\n",
    "#Using the RetrievalQA chain of Langchain to query from the index created\n",
    "MIN_DOCS = 1 \n",
    "  \n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\",\n",
    "                                retriever=docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": MIN_DOCS}))\n",
    "#query is inputted and run\n",
    "qa.run(query)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
