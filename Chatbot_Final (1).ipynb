{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "40TlGDEAVObZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfIF45HpU9ZG"
      },
      "outputs": [],
      "source": [
        "# Importing required modules, functions and libraries\n",
        "import os\n",
        "\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"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\"\n",
        "from langchain.llms import GPT4All\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain import HuggingFaceHub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id=\"tiiuae/falcon-7b-instruct\"\n",
        "embeddings = HuggingFaceEmbeddings() #Using the HuggingFace Embedding model, all-mpnet-base-v2\n",
        "llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\": 0.1}, verbose=True)\n",
        "# local_path = \"C:\\Programming\\GPT4All-13B-snoozy.ggmlv3.q4_0.bin\"  #Using GPT4ALL Snoozy Model, License - GPL\n",
        "# llm = GPT4All(model=local_path, verbose=True)"
      ],
      "metadata": {
        "id": "qfbpyAygVYKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class load_texts():\n",
        "    loader = TextLoader('documents.txt', encoding='UTF-8')\n",
        "    documents = loader.load()\n",
        "class splitting():\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=1)\n",
        "    texts = text_splitter.split_documents(load_texts.documents)\n"
      ],
      "metadata": {
        "id": "iHEHGvwHVaY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_index(embeddings, loader):\n",
        "    index = VectorstoreIndexCreator(embedding=embeddings,\n",
        "                                    vectorstore_kwargs={\"persist_directory\": \"test\"}\n",
        "                                    ).from_loaders([loader])\n",
        "    # In the above code, the embedding vectors are created, then the index is stored into a directory 'db'.\n",
        "    # The from_loaders method of the VectorstorIndexCreator object takes a list of loaders as in"
      ],
      "metadata": {
        "id": "6Ctp3Q_ufZ7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_index(embeddings, load_texts.loader)"
      ],
      "metadata": {
        "id": "yCn_Wv8HVgVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query = input(\"enter query\")\n",
        "texts = splitting.texts\n",
        "docsearch = Chroma.from_documents(texts, embeddings)\n",
        "\n",
        "#Using the RetrievalQA chain of Langchain to query from the index created\n",
        "MIN_DOCS = 1\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\",\n",
        "                                retriever=docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": MIN_DOCS}))\n",
        "#query is inputted and run\n",
        "\n",
        "qa.run(query)"
      ],
      "metadata": {
        "id": "Du4HnTBaVd15"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}